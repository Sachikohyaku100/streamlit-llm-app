import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
# from dotenv import load_dotenv  <- ã“ã®è¡Œã¯ä¸è¦ãªã®ã§å‰Šé™¤ã€ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ <- ã“ã®è¡Œã‚‚ä¸è¦ãªã®ã§å‰Šé™¤
# load_dotenv()

def get_llm_response(user_input, expert_choice):
    """
    LLMã‹ã‚‰ã®å›žç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°

    å¼•æ•°:
    - user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
    - expert_choice (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠžã—ãŸå°‚é–€å®¶

    æˆ»ã‚Šå€¤:
    - str: LLMã‹ã‚‰ã®å›žç­”
    """
    # â˜…â˜…â˜… ã“ã“ãŒå¤‰æ›´ç‚¹ â˜…â˜…â˜…
    # Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚“ã§æ¸¡ã™
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        openai_api_key=st.secrets["OPENAI_API_KEY"] # ã“ã®è¡Œã‚’è¿½åŠ 
    )

    # å°‚é–€å®¶ã®é¸æŠžã«å¿œã˜ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆAIã¸ã®æŒ‡ç¤ºï¼‰ã‚’å¤‰æ›´ã™ã‚‹
    if expert_choice == "ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶":
        system_message_content = "ã‚ãªãŸã¯å„ªç§€ãªãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚„é¡§å®¢ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®è¦³ç‚¹ã‹ã‚‰ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚"
    elif expert_choice == "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼":
        system_message_content = "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸã€å®‰å…¨ã§åŠ¹æžœçš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã‚„æ „é¤ŠæŒ‡å°Žã‚’ã—ã¦ãã ã•ã„ã€‚"
    elif expert_choice == "æ–™ç†ç ”ç©¶å®¶":
        system_message_content = "ã‚ãªãŸã¯å‰µé€ æ€§ã‚ãµã‚Œã‚‹æ–™ç†ç ”ç©¶å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æ‰‹è»½ã«ä½œã‚Œã‚‹ç¾Žå‘³ã—ã„ãƒ¬ã‚·ãƒ”ã‚„ã€æ–™ç†ãŒã‚‚ã£ã¨æ¥½ã—ããªã‚‹ã‚³ãƒ„ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æŒ‡ç¤º
        system_message_content = "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"

    # LLMã«æ¸¡ã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
    messages = [
        SystemMessage(content=system_message_content),
        HumanMessage(content=user_input),
    ]

    # LLMã‹ã‚‰å›žç­”ã‚’å–å¾—
    response = llm.invoke(messages)
    return response.content


# --- Streamlit ã‚¢ãƒ—ãƒªã®ç”»é¢æ§‹æˆ ---

# 1. ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜Ž
st.title("ðŸ¤– AIå°‚é–€å®¶ ãªã‚Šãã‚Šãƒãƒ£ãƒƒãƒˆ")
st.write("AIãŒæ§˜ã€…ãªåˆ†é‡Žã®å°‚é–€å®¶ã«ãªã‚Šãã£ã¦ã€ã‚ãªãŸã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚")
st.write("---")


# 2. å°‚é–€å®¶ã‚’é¸æŠžã™ã‚‹ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
expert_option = st.radio(
    label="ã©ã®å°‚é–€å®¶ã«ç›¸è«‡ã—ã¾ã™ã‹ï¼Ÿ",
    options=("ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶", "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼", "æ–™ç†ç ”ç©¶å®¶"),
    index=0, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é¸æŠž
    horizontal=True,
)


# 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ 
user_question = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...", height=150)


# 4. é€ä¿¡ãƒœã‚¿ãƒ³
if st.button("è³ªå•ã™ã‚‹"):
    if user_question:
        # è³ªå•ãŒã‚ã‚Œã°ã€é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å›žç­”ã‚’å–å¾—
        with st.spinner("AIãŒè€ƒãˆä¸­ã§ã™..."):
            answer = get_llm_response(user_question, expert_option)
            st.write("---")
            st.write(f"**{expert_option}ã‹ã‚‰ã®å›žç­”:**")
            st.write(answer)
    else:
        # è³ªå•ãŒãªã‘ã‚Œã°ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")